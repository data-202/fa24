[
  {
    "objectID": "weeks/week06.html",
    "href": "weeks/week06.html",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week06.html#part-i-context",
    "href": "weeks/week06.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week06.html#part-ii-content",
    "href": "weeks/week06.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatter plots\nCorrelation coefficients\nSimple linear regression\n\n\n\nSimple linear regression\nA simple linear regression (sometimes referenced as a bivariate regression) is a linear equation describing the relationship between an explanatory variable and an outcome variable.\nThere is an assumption that the explanatory variable influences the outcome variable, and not the other way around.\nTake, for example, a variable \\(y_i\\) which denotes the income of some individual in a sample, and we index this data using \\(i\\) where \\(i \\in \\{1, 2, ..., n\\}\\). We can let some other variable in our data \\(x_i\\) represent the years of education for the same individual. A simple linear regression equation of these variables take the following form: \\[y_i = b_0 + b_{1}x_{i} + e_i\\]\nwhere \\(b_1\\) is the sample estimate of the slope of the regression line with respect to the years of education and \\(b_0\\) is the sample estimate for the vertical intercept of the regression line.\n\n\n\nCorrelation coefficients and scatterplots\nAs a reminder, correlation ranges from -1 to 1. It gives us an indication on two things:\n\nThe direction of the relationship between the two variables\nThe strength of the relationship between the two variables\n\nAny outliers can greatly impact the value of a correlation coefficient.\n\nplot(x,y)\n\n\n\n\n\n\n\n\n\n\n\nEstimating differences in proportions\nWhen generating cross tabulations, we can make sense of a few bivariate tests:\n\nDifferences in proportions\nStandard errors of the difference in proportions\nConfidence intervals for the differences\nT-test for differences in proportions",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "weeks/week06.html#part-iii-code",
    "href": "weeks/week06.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\nThe code for this week will prepare you to run analyses for paper 2, which is a short exploration of a data set in the forcats package or other data sets that you select. Recall that lab 2 will guide you through your analyses; a sample RScript and workflow is also provided here.\n\n\nWrite preamble\n\n# ---\n# title: Exploring associations between income and political party in the US\n# subtitle: sample paper 2\n# author: Nathan Alexander\n# course: DATA 202 - fall 2023\n# ---\n\n# research inquiry: does income relate to political party support in the US?\n# data: 2020 sample data from the General Social Survey (GSS)\n# note(s): variables should be mutated and recoded into two levels\n\n\n\n\nstep 0: install packages and load libraries\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nstep 1: view gss_cat data in the package forcats\n\ngetwd() # check working directory\n?gss_cat # view data documentation\ngss_cat # call data frame\nglimpse(gss_cat) # glimpse data\nsummary(gss_cat) # view summary of data\n\n\n\n\nstep 2: clean and manage data\n\ngss_cat_clean &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  select(year, rincome, partyid) %&gt;% \n  rename(income = rincome) %&gt;% \n  rename(party = partyid)\n\ngss_cat_clean # view cleaned data\n\n\n\n\nstep 3: subset data: year == 2000\n\ndf &lt;- gss_cat_clean %&gt;% \n  filter(year==2000)\n\nhead(df) # view top of data\ntail(df) # view bottom of data\nsummary(df) # check data\n\n\n\n\nstep 4: inspect and transform income variable into two levels\n\n## create a frequency table of each level in the income variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Don't know', 'Refused', and 'Not applicable' levels\ndf &lt;- df %&gt;%  \n  filter(income != \"No answer\",\n         income != \"Don't know\",\n         income != \"Refused\",\n         income != \"Not applicable\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## use levels() function to view levels for income variable\nlevels(df$income)\n\n## create two levels: below $20,000 and above $20,000\ndf &lt;- df %&gt;% \n  mutate(income = fct_recode(income, \n                             \"More than 20000\" = \"$25000 or more\",\n                             \"More than 20000\" = \"$20000 - 24999\",\n                             \"Less than 20000\" = \"$15000 - 19999\",\n                             \"Less than 20000\" = \"$10000 - 14999\",\n                             \"Less than 20000\" = \"$8000 to 9999\",\n                             \"Less than 20000\" = \"$7000 to 7999\",\n                             \"Less than 20000\" = \"$6000 to 6999\",\n                             \"Less than 20000\" = \"$5000 to 5999\",\n                             \"Less than 20000\" = \"$4000 to 4999\",\n                             \"Less than 20000\" = \"$3000 to 3999\",\n                             \"Less than 20000\" = \"$1000 to 2999\",\n                             \"Less than 20000\" = \"Lt $1000\"))\n\n## view a summary of your transformed data frame\nsummary(df)\n\n\n\n\nstep 5: inspect and transform party variable into two levels\n\n## create a frequency table of each level in the party variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Independent' and 'Other Party' levels\ndf &lt;- df %&gt;%  \n  filter(party != \"No answer\",\n         party != \"Independent\",\n         party != \"Other party\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## create two levels: 'Republican' and 'Democrat'\ndf &lt;- df %&gt;% \n  mutate(party = fct_recode(party,\n                            \"Republican\" = \"Strong republican\",\n                            \"Republican\" = \"Not str republican\",\n                            \"Republican\" = \"Ind,near rep\",\n                            \"Democrat\" = \"Ind,near dem\",\n                            \"Democrat\" = \"Not str democrat\",\n                            \"Democrat\" = \"Strong democrat\"))\n\n## remove year from data frame\ndf &lt;- df %&gt;% \n  select(-year)\n\n## view a summary of the data to check for any errors\nsummary(df)\n\n\n\n\nstep 6: visualize data\n\n## create a frequency table and bar graph of income \ntable.income = table(df$income)\ntable.income\nbarplot(table.income,\n        main = \"Bar graph of Income\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$income),\n        main = \"Bar graph of Income\",\n        col = \"lightgreen\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\",\n        ylim = c(0,650)) # this y-axis range: (0, 650) works best for my plot\n## create a frequency table and bar graph of party \ntable.party = table(df$party)\ntable.party\nbarplot(table.party,\n        main = \"Bar graph of Party\",\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$party),\n        main = \"Bar graph of Party\",\n        col = c(\"red\",\"blue\"),\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\",\n        ylim = c(0,600)) # this y-axis range: (0, 550) works best for my plot\n## create a stacked bar plot of the proportions\n#### question: which of the two plots do you prefer, why?\nplot(df$income, df$party)\nplot(df$party, df$income)\n## the below code produces similar outputs as above with specifications\nplot(df$party, df$income,\n     main = \"Mosaic Plot of Political Party and Income\",\n     col = c(\"lightyellow\",\"lightgreen\"),\n     xlab = \"Political Party\",\n     ylab = \"Income\")\n\n\n\n\nstep 7: create a basic cross tab for manual calculations\n\n## gather sample size\nn = count(df)\nn\n\n## view a basic cross tabulation\ntable(df$income, df$party)\n\n\n\n\nstep 8: bivariate analysis\n\n## load required libraries (and packages, where needed)\ninstall.packages(\"descr\", repos = \"http://cran.us.r-project.org\")\nlibrary(descr)\n\ninstall.packages(\"Hmisc\", repos = \"http://cran.us.r-project.org\")\nlibrary(Hmisc)\n\n## create a cross tab (list dependent variable in your hypothesis first)\ncrosstab(df$party, df$income)\n\n## add column percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.c=T) # add column percentages\n## add row percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.r=T) # add row percentages\n\n## get expected frequencies and cell chi-square contributions\ncrosstab(df$party, df$income,\n         expected = T, # get expected values\n         prop.chisq=T) # get chi-square contribution\n\n## get critical value of chi-square, p=.05, df=1\n#### recall: df = (r-1)(c-1)\nqchisq(.05, 1, lower.tail=F)\n\n## get chi-square statistic\nchisq.test(df$party, df$income)\n\n\n\n\nstep 9: describe some initial limitations of analysis\n\n### limitation 1: sampling error\n# data come from a sample and there are likely differences in other samples\n\n### limitation 2: category reductions\n# creating two levels for the variables greatly impacted the diversity of responses\n\n### limitation 3: cases dropped\n# sample was further impacted by the number of values dropped in the analysis\n\n### limitation 4: chi-square test\n# the chi-square test does not tell us about the strength or direction of association\n\n\n\n\nNext up: Week 7",
    "crumbs": [
      "Weekly materials",
      "Week 6"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Part\nWeek\nDay\nTopics\nLabs\nPapers\nKey readings, exams, other\n\n\n\n\nI\n1\nWed, Aug 21\nIntroduction\nLab 0\n\nCase study 1 files\n\n\n\n2\nWed, Aug 28\nFoundations\n\n\nPelham (2013, ch. 1)\n\n\n\n3\nWed, Sept 4\nTheory construction\n\n\nAnnotated bibliography\n\n\n\n4\nWed, Sept 11\nProbability theory\nLab 1\n\nShore (2008); Zuberi (2000)\n\n\nII\n5\nWed, Sept 18\nUnivariate analysis\n\n\nWilson et al. (2017)\n\n\n\n6\nWed, Sept 25\nBivariate analysis\n\nPaper 1\n\n\n\n\n7\nWed, Oct 2\n\n\n\n\n\n\n\n8\nWed, Oct 9\nExploratory analysis\n\n\n\n\n\nIII\n9\nWed, Oct 16\nHypothesis testing\n\n\n\n\n\n\n10\nWed, Oct 23\nBivariate regression analysis\nLab 2\n\n\n\n\n\n11\nWed, Oct 30\nMultivariate regression analysis\n\n\n\n\n\n\n12\nWed, Nov 6\nMultiple variable regression\n\nPaper 2\n\n\n\nIV\n13\nWed, Nov 13\nRegression and classification\nLab 3\n\n\n\n\n\n14\nWed, Nov 20\nStatistical learning\n\n\n\n\n\n\n15\nWed, Nov 27\nOverview of additional models\n\nPaper 3",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "computing.html#setting-up-your-reference-and-style-files.",
    "href": "computing.html#setting-up-your-reference-and-style-files.",
    "title": "Computing",
    "section": "Setting up your reference and style files.",
    "text": "Setting up your reference and style files.",
    "crumbs": [
      "Course information",
      "Computing"
    ]
  },
  {
    "objectID": "computing.html#lab-0-downloading-r-and-rstudio.",
    "href": "computing.html#lab-0-downloading-r-and-rstudio.",
    "title": "Computing",
    "section": "Lab 0: Downloading R and RStudio.",
    "text": "Lab 0: Downloading R and RStudio.\nFor Lab #0 you will download two software programs to your computer.\nPlease watch the below video to help you download R and RStudio.\n\n\n\n\nWatch this video to complete the steps below:\n\n- Step 1: Navigate to https://posit.co\n\n\n- Step 2: Click on the Download RStudio link\n\n\n- Step 3: Install base R (Install R)\n\n\n- Step 4: Install the RStudio IDE (Install RStudio)\nOnce you have completed your downloads, check to see that you can open RStudio (not R).\nWe’ll pick up here in class next week.\n\n\nNext up: Schedule and Week 1\nPlease see the full course schedule on the next page.\nAs a thought exercise, you will write a short paper based on a social justice issue you are familiar with or wish to learn more about. In this paper, you should write about a theory concerning the relationships between and among some dependent variable and particular independent variables (you may have one or more than one independent variables of interest at the outset). After introducing your theory and assuming you had unlimited research funding, propose a research design and measurement plan for a study that would use real-world data and statistical analysis to examine the theory.",
    "crumbs": [
      "Course information",
      "Computing"
    ]
  },
  {
    "objectID": "papers/papers-more-info.html",
    "href": "papers/papers-more-info.html",
    "title": "More information for course papers",
    "section": "",
    "text": "Overview\nAs noted before, the course papers all require that you do original data analysis.\nPapers should be written as short research reports (try to make the text clear but sound professional, though the research questions may be relatively simple), and they should include the elements described below in narrative form. Please give each paper a substantive title (in addition to “Paper #_“).\nEach paper should have, at minimum, four sections: Theory, Measurement, Statistical Analysis, and Conclusion.\n\n\nTheory:\nEach theory (or logic graph) should be described fully in about a paragraph or two.\nDescribe and justify the relationships involving the theoretical variables that will be examined (including what’s the unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work). Include a graph along with this at the beginning of the paper.\n\n\nMeasurement:\nNext, describe the data that you will be using to examine the theory. Describe the data source that you are using, and for any survey data describe the sampling method. For the General Social Survey (GSS) someone could say something like “The 2012 GSS sample is a multistage area probability sample to the segment or block level. At the block level, households are enumerated and a full probability sample is drawn.” You will need to find this type of detailed information in the Data Codebooks provided on Canvas. Describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and re-coded. Present the frequency distributions (so your coding and treatment of missing data can be checked); if you want to comment on the frequency distributions, you should do so in no more than a couple sentences. Please include all the R commands that you executed in a .qmd file, so your work can be checked (or the commands for whatever code you use in R).\n\n\nStatistical Analysis:\nSpecify the hypotheses for the expected relationships (coefficients / correlations) in the context of the specified equations and operational flow graph. Then report the statistical results, coefficients, etc. and other relevant statistics and any additional calculations required. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing.\n\n\nConclusion:\nWhat can you conclude about your original theorizing based upon your analysis? Were you on the right track? Were your findings different from what you originally theorized?",
    "crumbs": [
      "Appendix",
      "Papers",
      "More information for papers"
    ]
  },
  {
    "objectID": "papers/paper2.html",
    "href": "papers/paper2.html",
    "title": "Paper 2",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#path-diagram",
    "href": "papers/paper2.html#path-diagram",
    "title": "Paper 2",
    "section": "Path Diagram",
    "text": "Path Diagram\n\n\n\n\n\nflowchart LR\n  A[Income] --&gt; B[Strength of support for a political party]",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#prepare-files",
    "href": "papers/paper2.html#prepare-files",
    "title": "Paper 2",
    "section": "Prepare files",
    "text": "Prepare files\nTo investigate the relationship outlined in the path diagram, we will use data from the General Social Survey (GSS). To begin your exploratory analysis, start a new RScript in your stats-pt2 RStudio project directory and give your RScript a proper preamble.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#data-set",
    "href": "papers/paper2.html#data-set",
    "title": "Paper 2",
    "section": "Data set",
    "text": "Data set\nFor paper #2, you will use the gss_cat data located in the forcats package.\nTo test our hypothesis, two survey variables/measures are selected from the gss_cat data:\n\nrincome (respondent’s reported income)\npartyid (respondent’s levels of support for one of three major US political parties)\n\n\nLoad the libraries\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidycensus)\n\n\n\nView data in your current R session\n\ndata()\n\nLocate the gss_cat data in the forcats pacakge.\nThis is a data set for us to examine the use of categorical data and factor variable types.\n\n\nView documentation for your data\n\n?gss_cat\n\nView your data\n\ngss_cat\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Ind,near … Prot… Sout…      12\n 2  2000 Divorced         48 White $8000 to 9999  Not str r… Prot… Bapt…      NA\n 3  2000 Widowed          67 White Not applicable Independe… Prot… No d…       2\n 4  2000 Never married    39 White Not applicable Ind,near … Orth… Not …       4\n 5  2000 Divorced         25 White Not applicable Not str d… None  Not …       1\n 6  2000 Married          25 White $20000 - 24999 Strong de… Prot… Sout…      NA\n 7  2000 Never married    36 White $25000 or more Not str r… Chri… Not …       3\n 8  2000 Divorced         44 White $7000 to 7999  Ind,near … Prot… Luth…      NA\n 9  2000 Married          44 White $25000 or more Not str d… Prot… Other       0\n10  2000 Married          47 White $25000 or more Strong re… Prot… Sout…       3\n# ℹ 21,473 more rows\n\n\nBe sure to check the categories and codes for your variables.\n\nCategories for rincome\n\nsummary(gss_cat$rincome)\n\n     No answer     Don't know        Refused $25000 or more $20000 - 24999 \n           183            267            975           7363           1283 \n$15000 - 19999 $10000 - 14999  $8000 to 9999  $7000 to 7999  $6000 to 6999 \n          1048           1168            340            188            215 \n $5000 to 5999  $4000 to 4999  $3000 to 3999  $1000 to 2999       Lt $1000 \n           227            226            276            395            286 \nNot applicable \n          7043 \n\n\n\n\nCategories for partyid\n\nsummary(gss_cat$partyid)\n\n         No answer         Don't know        Other party  Strong republican \n               154                  1                393               2314 \nNot str republican       Ind,near rep        Independent       Ind,near dem \n              3032               1791               4119               2499 \n  Not str democrat    Strong democrat \n              3690               3490 \n\n\nNotice how each variable is measured and take notes for the measurement section of your paper.\nYou will need to re-code these values prior to any statistical analysis.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#prepare-data-for-analysis",
    "href": "papers/paper2.html#prepare-data-for-analysis",
    "title": "Paper 2",
    "section": "Prepare data for analysis",
    "text": "Prepare data for analysis\nBegin exploratory analyses on each variable.\nWhile doing exploratory analyses, you should prepare your data for statistical analysis.\n\nInspect your data using str()\nCheck the distribution of your variables using count()\nDecide how you will work with missing data, such as na.omit()\n\n\nRemove missing values from your analysis\nFor this paper, it is fine to drop all missing observations.\nWe will also explore the data from the year 2000.\n\ndf &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  filter(year == 2000) %&gt;% \n  select(year, rincome, partyid)\n\nExamine the reduced data frame.\n\ndf\n\n# A tibble: 1,824 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 Not applicable Independent       \n 3  2000 Not applicable Ind,near rep      \n 4  2000 Not applicable Not str democrat  \n 5  2000 $25000 or more Not str republican\n 6  2000 $25000 or more Not str democrat  \n 7  2000 $25000 or more Strong republican \n 8  2000 $25000 or more Not str democrat  \n 9  2000 $25000 or more Strong democrat   \n10  2000 $25000 or more Ind,near dem      \n# ℹ 1,814 more rows\n\n\nView the top and bottom of your data using head and tail, respectively.\n\nhead(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid           \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n1  2000 $8000 to 9999  Ind,near rep      \n2  2000 Not applicable Independent       \n3  2000 Not applicable Ind,near rep      \n4  2000 Not applicable Not str democrat  \n5  2000 $25000 or more Not str republican\n6  2000 $25000 or more Not str democrat  \n\ntail(df)\n\n# A tibble: 6 × 3\n   year rincome        partyid          \n  &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;            \n1  2000 $25000 or more Not str democrat \n2  2000 $25000 or more Strong republican\n3  2000 $10000 - 14999 Independent      \n4  2000 $25000 or more Strong republican\n5  2000 $25000 or more Ind,near rep     \n6  2000 $7000 to 7999  Strong republican\n\n\nIn paper 2, you will conduct original statistical analyses.\n\n\nGet frequency tables for your data\nUsing the count() function, we will get frequency tables for each variable.\n\ndf %&gt;% \n  count(rincome)\n\n# A tibble: 16 × 2\n   rincome            n\n   &lt;fct&gt;          &lt;int&gt;\n 1 No answer         15\n 2 Don't know        22\n 3 Refused           92\n 4 $25000 or more   590\n 5 $20000 - 24999   126\n 6 $15000 - 19999   115\n 7 $10000 - 14999   139\n 8 $8000 to 9999     45\n 9 $7000 to 7999     18\n10 $6000 to 6999     24\n11 $5000 to 5999     17\n12 $4000 to 4999     20\n13 $3000 to 3999     22\n14 $1000 to 2999     36\n15 Lt $1000          24\n16 Not applicable   519\n\n\n\ndf %&gt;% \n  count(partyid)\n\n# A tibble: 9 × 2\n  partyid                n\n  &lt;fct&gt;              &lt;int&gt;\n1 No answer              3\n2 Other party           22\n3 Strong republican    180\n4 Not str republican   244\n5 Ind,near rep         168\n6 Independent          392\n7 Ind,near dem         213\n8 Not str democrat     331\n9 Strong democrat      271\n\n\nFrom our frequency tables, it is clear that we will need to transform our data.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "href": "papers/paper2.html#recode-categories-into-two-levels-dichotomous",
    "title": "Paper 2",
    "section": "Recode categories into two-levels (dichotomous)",
    "text": "Recode categories into two-levels (dichotomous)\nFor this analysis, we will transform our data into two dichotomous variables. Let’s examine the outputs before overwriting our data frame.\nWe will use the logic operator != to imply we do not want to keep these values (i.e., we will filter the values that are not equal to the right hand side).\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\")\n\n# A tibble: 1,198 × 3\n    year rincome        partyid           \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;             \n 1  2000 $8000 to 9999  Ind,near rep      \n 2  2000 $25000 or more Not str republican\n 3  2000 $25000 or more Not str democrat  \n 4  2000 $25000 or more Strong republican \n 5  2000 $25000 or more Not str democrat  \n 6  2000 $25000 or more Strong democrat   \n 7  2000 $25000 or more Ind,near dem      \n 8  2000 $25000 or more Strong democrat   \n 9  2000 $25000 or more Independent       \n10  2000 $10000 - 14999 Not str democrat  \n# ℹ 1,188 more rows\n\n\nWe now recode the categories using mutuate() and recode().\n\ndf %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 10000\" = \"$25000 or more\",\n          \"More than 10000\" = \"$20000 to 24999\",\n          \"More than 10000\" = \"$15000 to 19999\",\n          \"More than 10000\" = \"$10000 to 14999\",\n          \"Less than 10000\" = \"$8000 to 9999\",\n          \"Less than 10000\" = \"$7000 to 7999\",\n          \"Less than 10000\" = \"$6000 to 6999\",\n          \"Less than 10000\" = \"$5000 to 5999\",\n          \"Less than 10000\" = \"$4000 to 4999\",\n          \"Less than 10000\" = \"$3000 to 3999\",\n          \"Less than 10000\" = \"$1000 to 2999\",\n          \"Less than 10000\" = \"$Lt $1000\"))\n\n# A tibble: 1,824 × 3\n    year rincome         partyid           \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;             \n 1  2000 Less than 10000 Ind,near rep      \n 2  2000 Not applicable  Independent       \n 3  2000 Not applicable  Ind,near rep      \n 4  2000 Not applicable  Not str democrat  \n 5  2000 More than 10000 Not str republican\n 6  2000 More than 10000 Not str democrat  \n 7  2000 More than 10000 Strong republican \n 8  2000 More than 10000 Not str democrat  \n 9  2000 More than 10000 Strong democrat   \n10  2000 More than 10000 Ind,near dem      \n# ℹ 1,814 more rows\n\n\n\ndf %&gt;% \n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,824 × 3\n    year rincome        partyid    \n   &lt;int&gt; &lt;fct&gt;          &lt;fct&gt;      \n 1  2000 $8000 to 9999  Republican \n 2  2000 Not applicable Independent\n 3  2000 Not applicable Republican \n 4  2000 Not applicable Democrat   \n 5  2000 $25000 or more Republican \n 6  2000 $25000 or more Democrat   \n 7  2000 $25000 or more Republican \n 8  2000 $25000 or more Democrat   \n 9  2000 $25000 or more Democrat   \n10  2000 $25000 or more Democrat   \n# ℹ 1,814 more rows\n\n\nWe can stack our variable transformations together into one chunk of code.\nTake note of the way we are creating our new data set for analysis.\n\ndf %&gt;% \n  filter(year == 2000) %&gt;% \n  filter(rincome != \"No answer\"\n         & rincome != \"Refused\"\n         & rincome != \"Not applicable\") %&gt;% \n  mutate(rincome = fct_recode(rincome, \n          \"More than 20000\" = \"$25000 or more\",\n          \"More than 20000\" = \"$20000 - 24999\",\n          \"Less than 20000\" = \"$15000 - 19999\",\n          \"Less than 20000\" = \"$10000 - 14999\",\n          \"Less than 20000\" = \"$8000 to 9999\",\n          \"Less than 20000\" = \"$7000 to 7999\",\n          \"Less than 20000\" = \"$6000 to 6999\",\n          \"Less than 20000\" = \"$5000 to 5999\",\n          \"Less than 20000\" = \"$4000 to 4999\",\n          \"Less than 20000\" = \"$3000 to 3999\",\n          \"Less than 20000\" = \"$1000 to 2999\",\n          \"Less than 20000\" = \"Lt $1000\")) %&gt;%\n  mutate(partyid = fct_recode(partyid,\n                              \"Republican\" = \"Strong republican\",\n                              \"Republican\" = \"Not str republican\",\n                              \"Republican\" = \"Ind,near rep\",\n                              \"Democrat\" = \"Ind,near dem\",\n                              \"Democrat\" = \"Not str democrat\",\n                              \"Democrat\" = \"Strong democrat\"))\n\n# A tibble: 1,198 × 3\n    year rincome         partyid    \n   &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;      \n 1  2000 Less than 20000 Republican \n 2  2000 More than 20000 Republican \n 3  2000 More than 20000 Democrat   \n 4  2000 More than 20000 Republican \n 5  2000 More than 20000 Democrat   \n 6  2000 More than 20000 Democrat   \n 7  2000 More than 20000 Democrat   \n 8  2000 More than 20000 Democrat   \n 9  2000 More than 20000 Independent\n10  2000 Less than 20000 Democrat   \n# ℹ 1,188 more rows\n\n\nAs you examine the code more closely, you will notice that I created two categories:\n\nPeople making less than $20,000\nPeople making more than $20,000",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 2"
    ]
  },
  {
    "objectID": "papers/paper1.html",
    "href": "papers/paper1.html",
    "title": "Paper 1",
    "section": "",
    "text": "As a thought exercise for your first assignment, you will write a short quantitative research proposal based on a social issue you are familiar with or wish to learn more about.\nIn this paper, you should write about a theory concerning the relationships between and among some dependent variable and particular independent variables (you may have one or more than one independent variables of interest at the outset). After introducing your theory and assuming you had unlimited research funding, propose a research design and measurement plan for a study that would use real-world data and statistical analysis to examine the theory.\nFor review, you may also refer to more information for course papers in two additional documents:\nYour paper should be clear on the following:",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "papers/paper1.html#paper-requirements",
    "href": "papers/paper1.html#paper-requirements",
    "title": "Paper 1",
    "section": "Paper requirements",
    "text": "Paper requirements\nThis is an individual paper. Your paper should be no longer than five (5) pages in length and written as a narrative essay in 12-point Times New Roman font with one-inch margins.\nIn writing the paper, you should structure it by using section headings and relevant citations for any referenced work.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "papers/paper1.html#resource",
    "href": "papers/paper1.html#resource",
    "title": "Paper 1",
    "section": "Resource",
    "text": "Resource\nSpencer Foundation (2020). A Guide to Quantitative Research Proposals",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 1"
    ]
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Statistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\n\n\n\nDEFINITION: Statistics\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, statistics calls for questions where we explore difference or change. We use variation to understand differences within or between a set (or sets) of measurements.\n\n\n\n“Statistics are a set of mathematical procedures for summarizing and interpreting observations” (p.5).\nDescriptive statistics vs. Inferential statistics\nPelham describes descriptive statistics as “statistics used to summarize or describe a set of observations”, whereas inferential statistics are defined as “statistics used to draw inferences about a set of observations” (p. 5).\n\n\n\n\nCentral tendency: the “average” of a set of observations (i.e., mean, median, mode)\nDispersion: how data vary (i.e., range, variance, standard deviation)\n\n\n\nAssume we have a small set of data: 2, 2, 4, 4. We can input and compute this data in R using the following code:\n\n# Generate sample data\nx &lt;- c(2, 2, 4, 4)\n\n\n\n# compute the mean\nm = sum(x)/4\nm\n\n[1] 3\n\n\n\n\n# compute the median\nmedian(x)\n\n[1] 3\n\n\n\n\n# compute the mode\nmode(x)\n\n[1] \"numeric\"\n\ntable.x &lt;- table(x)\ntable.x\n\nx\n2 4 \n2 2 \n\n\n\n\n# create a histogram\nhist(x)\n\n\n\n\n\n\n\n\n\nYou also may recall the standard deviation, \\(S\\), of a set of values is calculated using: \\[ S = \\sqrt{\\dfrac{\\Sigma (x - m)^2}{n}}\\]\n\n\n\nAssume we have a small set of data: 2, 2, 4, 4. Using the formula for standard deviation above, where \\(x\\) is the score, \\(m\\) is the mean (sometimes referred to as \\(\\bar{x}\\)), and \\(n\\) is the sample size, we get the following:\n\\[\\dfrac{(2-3)^2 + (2-3)^2 + (4-3)^2 + (4-3)^2}{4} = \\dfrac{1 + 1 + 1 + 1}{4} = 1 \\] Most formulas of this nature are programmed into R/RStudio, but for more advanced modeling, we learn how to input raw data and formula into the software.\n\n\n\n\n\nPelham notes that “the basic idea behind inferential statistical testing is that decisions about what to conclude from a set of research findings need to be made in a logical, unbiased fashoion” (p. 13). However, with a more critical lens on the development of statistics and some of the assumptions made in the mathematical models, we need to consider more concretely about the role of theory and context in making inferences about data.\n\n\n\n\n\n\n\n\nOn Canvas, in the Week 2 folder, there is a document titled “Critical Thinking” by Jennifer Duncan. This document is one example of how we can frame what it could or should mean to be critical in statistics. Please review this document.\n\n\nUsing a higher order of thinking. Duncan emphasizes that critical thinking is a higher order of thinking with different advanced thinking skills, and offers a few suggestions.\n\nWe base our thinking on logic and not on feelings.\nWe should look deeper into inferences for hidden assumptions or values.\nAsk complex questions that help build a critical inquiry.\n\n\n\n\nAsking complex questions. Duncan breaks down the process into a few sub-questions.\n\nWho is the implied audience?\nWhat are the strengths and weaknesses of the argument?\nWhat are the underlying assumptions and values?\n\n\n\n\nUsing a variety of thinking processes. Duncan defines a process around analyzing, synthesizing, interpreting, and evaluating information that helps with our thinking.\n\n\n\nReflecting on how we answer a question. Duncan ends with a set of questions that help us think about different points of view, if we have clarity, and if more details are needed.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#what-is-statistics",
    "href": "weeks/week02.html#what-is-statistics",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "Statistics is a science. As a result, it follows a set of well-defined steps or methods. As we explore new terms and definitions, we will gain a better understanding of what statistics encompasses.\n\n\n\n\n\n\nDEFINITION: Statistics\n\n\n\nStatistics is the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\n\n\nThere are a multitude of ways to describe the steps, terms, and various processes undertaken in a statistical study. Importantly, statistics calls for questions where we explore difference or change. We use variation to understand differences within or between a set (or sets) of measurements.\n\n\n\n“Statistics are a set of mathematical procedures for summarizing and interpreting observations” (p.5).\nDescriptive statistics vs. Inferential statistics\nPelham describes descriptive statistics as “statistics used to summarize or describe a set of observations”, whereas inferential statistics are defined as “statistics used to draw inferences about a set of observations” (p. 5).\n\n\n\n\nCentral tendency: the “average” of a set of observations (i.e., mean, median, mode)\nDispersion: how data vary (i.e., range, variance, standard deviation)\n\n\n\nAssume we have a small set of data: 2, 2, 4, 4. We can input and compute this data in R using the following code:\n\n# Generate sample data\nx &lt;- c(2, 2, 4, 4)\n\n\n\n# compute the mean\nm = sum(x)/4\nm\n\n[1] 3\n\n\n\n\n# compute the median\nmedian(x)\n\n[1] 3\n\n\n\n\n# compute the mode\nmode(x)\n\n[1] \"numeric\"\n\ntable.x &lt;- table(x)\ntable.x\n\nx\n2 4 \n2 2 \n\n\n\n\n# create a histogram\nhist(x)\n\n\n\n\n\n\n\n\n\nYou also may recall the standard deviation, \\(S\\), of a set of values is calculated using: \\[ S = \\sqrt{\\dfrac{\\Sigma (x - m)^2}{n}}\\]\n\n\n\nAssume we have a small set of data: 2, 2, 4, 4. Using the formula for standard deviation above, where \\(x\\) is the score, \\(m\\) is the mean (sometimes referred to as \\(\\bar{x}\\)), and \\(n\\) is the sample size, we get the following:\n\\[\\dfrac{(2-3)^2 + (2-3)^2 + (4-3)^2 + (4-3)^2}{4} = \\dfrac{1 + 1 + 1 + 1}{4} = 1 \\] Most formulas of this nature are programmed into R/RStudio, but for more advanced modeling, we learn how to input raw data and formula into the software.\n\n\n\n\n\nPelham notes that “the basic idea behind inferential statistical testing is that decisions about what to conclude from a set of research findings need to be made in a logical, unbiased fashoion” (p. 13). However, with a more critical lens on the development of statistics and some of the assumptions made in the mathematical models, we need to consider more concretely about the role of theory and context in making inferences about data.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "href": "weeks/week02.html#what-should-it-mean-to-be-critical-in-the-context-of-statistics",
    "title": "DATA 202 - Week 2",
    "section": "",
    "text": "On Canvas, in the Week 2 folder, there is a document titled “Critical Thinking” by Jennifer Duncan. This document is one example of how we can frame what it could or should mean to be critical in statistics. Please review this document.\n\n\nUsing a higher order of thinking. Duncan emphasizes that critical thinking is a higher order of thinking with different advanced thinking skills, and offers a few suggestions.\n\nWe base our thinking on logic and not on feelings.\nWe should look deeper into inferences for hidden assumptions or values.\nAsk complex questions that help build a critical inquiry.\n\n\n\n\nAsking complex questions. Duncan breaks down the process into a few sub-questions.\n\nWho is the implied audience?\nWhat are the strengths and weaknesses of the argument?\nWhat are the underlying assumptions and values?\n\n\n\n\nUsing a variety of thinking processes. Duncan defines a process around analyzing, synthesizing, interpreting, and evaluating information that helps with our thinking.\n\n\n\nReflecting on how we answer a question. Duncan ends with a set of questions that help us think about different points of view, if we have clarity, and if more details are needed.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#sets-and-numbers",
    "href": "weeks/week02.html#sets-and-numbers",
    "title": "DATA 202 - Week 2",
    "section": "Sets and numbers",
    "text": "Sets and numbers\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers\n\n\n\n– Natural numbers: \\(\\mathbb{N} = \\{1, 2, 3, ...\\}\\)\n– Whole numbers: \\(\\mathbb{N_0} = \\{0, 1, 2, 3, ...\\}\\)\n– Integers: \\(\\mathbb{Z} = \\{..., -3, -2, -1, 0, 1, 2, 3, ...\\}\\)\n– Rational numbers: \\(\\mathbb{Q} = \\Big\\{\\dfrac{p}{q}, p \\in \\mathbb{Z}, q \\in \\mathbb{Z}, q \\neq 0 \\Big\\}\\)\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Sets of numbers, continued…\n\n\n\n\nIrrational numbers\n\nany number that is not a rational number; irrational means not rational (no ratio)\ne.g., you may know some irrational numbers such as \\(\\pi\\), \\(\\sqrt{2}\\), \\(\\sqrt{3}\\), \\(e\\) (Euler’s number)\n\nReal numbers: \\(\\mathbb{R}\\)\n\nThe set of numbers on the real number line\nThis set is constructed by combining the rational and irrational numbers\n\nImaginary numbers: \\(\\mathbb{I}\\)\n\na number that has a negative value when it is squared\n\\(i\\) is the unit imaginary number, \\(\\sqrt{-1} = i\\) by definition\nso \\(i\\) is a complex number since \\(i^2 = -1\\)\n\nComplex numbers: \\(\\mathbb{C}\\)\n\na number in the form \\(a + bi\\) where \\(a \\in \\mathbb{R}\\), \\(b \\in \\mathbb{R}\\), and \\(i\\) is an imaginary number",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#getting-started-in-rstudio",
    "href": "weeks/week02.html#getting-started-in-rstudio",
    "title": "DATA 202 - Week 2",
    "section": "Getting started in RStudio",
    "text": "Getting started in RStudio\nIn Lab 0, you downloaded and installed base R and RStudio. In this section, we will learn more about R and RStudio.\nLet’s start with a little fun!\n\nFirst, install the ‘praise’ package.\n\n# install the package\ninstall.packages(\"praise\", repos = \"http://cran.us.r-project.org\")\n\n\nNext, load the library for the ‘praise’ package.\n\n# load library\nlibrary(praise)\n\n\nNow, get some praise!\n\n# get some praise\npraise()\n\nYou can keep inserting the code above to get praise when you need it!",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#arithmetic-in-r",
    "href": "weeks/week02.html#arithmetic-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Arithmetic in R",
    "text": "Arithmetic in R\nWe will learn how to calculate values in R.\n\n\n1 + 2  # the 'plus sign' computes the sum\n\n[1] 3\n\n\n\n\n2 - 3  # the 'minus sign' computes the difference\n\n[1] -1\n\n\n\n\n3 * 4  # the 'asterisk' computes the product\n\n[1] 12\n\n\n\n\n4 / 5 # the 'forward slash' computes the quotient\n\n[1] 0.8\n\n\n\n\n# from hw exercise 0.2, we can compute the sum of the first 100 positive integers\nsum(1:100) \n\n[1] 5050",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#variables-in-r",
    "href": "weeks/week02.html#variables-in-r",
    "title": "DATA 202 - Week 2",
    "section": "Variables in R",
    "text": "Variables in R\nWe will learn to give a variable (or character) a value.\n\nUse the different assignment operators\n\ny = 2 # the equal sign can be used as an assignment operator\n\ny &lt;-2 # a \"less than\" sign and dash can also be used as an assignment operator\n\ny # R stores all values you assign, so you must \"call\" any variables to see their values\n\n[1] 2\n\n\n\nSet x equal to two added to three\n\nx = 2 + 3\nx\n\n[1] 5\n\n\n\nSet y equal to two minus three\n\ny = 2 - 3\ny\n\n[1] -1\n\n\n\nSet z equal to two times three\n\nz = 2 * 3\nz\n\n[1] 6\n\n\n\nOverwrite the value of y by setting y equal to x divided by z\n\ny = x / z\ny\n\n[1] 0.8333333\n\n\n\n\n\n\n\n\n\nPaper 1 is due on Monday September 16 at 11:59pm ET\n\n\n\n\nIn the next module, we will continue our explorations in R by learning how to load data sets into our data frame, and perform some basic operations using some additional packages. These packages will allow us to consider how we can construct original data sets to develop unique questions for our analysis.",
    "crumbs": [
      "Weekly materials",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week15.html",
    "href": "weeks/week15.html",
    "title": "DATA 202 - Week 15",
    "section": "",
    "text": "Review Paper 4 assignment (final assessment).\n\n\n\nThe rubric for the final assessment (paper 4) will be used to assess your growth over the course of the semester. In addition to the requirements outlined in the assignment, the rubric will be used to assess the overall quality of the submission.\n\nDATA 202 final assessment rubric\n\n\n\n\n\n\n\n\n\nCriteria\nExemplary\nGood\nAcceptable\nIncomplete\n\n\n\n\nPurpose\nThe central purpose or argument is readily apparent.\nThe writing has a clear purpose or argument, but may sometimes digress.\nThe central purpose or argument is not consistently clear throughout the paper.\nThe purpose or argument is generally unclear.\n\n\nContent\nThe focus of the paper relates to a relevant social issue and includes citations and support from the literature.\nThe content of the paper provides reasonable support for a social issue but may sometimes be unclear.\nThe content of the paper is not consistent in its relation to a social issue.\nThe content of the paper is generally not related to a social issue.\n\n\nOrganization\nThe ideas are arranged logically and the paper contains the required sections as outlined in the paper assignment.\nThe ideas are arranged logically for the most part but may not include all required details or sections.\nThe ideas are arranged in some logical fashion but fail to provide the required details and sections.\nThe ideas are not arranged logically and fail to attend to the required sections as outlined in the paper assignment.\n\n\nR Code\nAll code is original and directly relates to the logic model and theory.\nThe code is original but may sometimes digress from the logic model and theory.\nThe code is somewhat original but fails to to relate to the logic model or theory.\nThe code is not original and does not relate to the logical model and theory.",
    "crumbs": [
      "Weekly materials",
      "Week 15"
    ]
  },
  {
    "objectID": "weeks/week15.html#part-i-content",
    "href": "weeks/week15.html#part-i-content",
    "title": "DATA 202 - Week 15",
    "section": "",
    "text": "Review Paper 4 assignment (final assessment).\n\n\n\nThe rubric for the final assessment (paper 4) will be used to assess your growth over the course of the semester. In addition to the requirements outlined in the assignment, the rubric will be used to assess the overall quality of the submission.\n\nDATA 202 final assessment rubric\n\n\n\n\n\n\n\n\n\nCriteria\nExemplary\nGood\nAcceptable\nIncomplete\n\n\n\n\nPurpose\nThe central purpose or argument is readily apparent.\nThe writing has a clear purpose or argument, but may sometimes digress.\nThe central purpose or argument is not consistently clear throughout the paper.\nThe purpose or argument is generally unclear.\n\n\nContent\nThe focus of the paper relates to a relevant social issue and includes citations and support from the literature.\nThe content of the paper provides reasonable support for a social issue but may sometimes be unclear.\nThe content of the paper is not consistent in its relation to a social issue.\nThe content of the paper is generally not related to a social issue.\n\n\nOrganization\nThe ideas are arranged logically and the paper contains the required sections as outlined in the paper assignment.\nThe ideas are arranged logically for the most part but may not include all required details or sections.\nThe ideas are arranged in some logical fashion but fail to provide the required details and sections.\nThe ideas are not arranged logically and fail to attend to the required sections as outlined in the paper assignment.\n\n\nR Code\nAll code is original and directly relates to the logic model and theory.\nThe code is original but may sometimes digress from the logic model and theory.\nThe code is somewhat original but fails to to relate to the logic model or theory.\nThe code is not original and does not relate to the logical model and theory.",
    "crumbs": [
      "Weekly materials",
      "Week 15"
    ]
  },
  {
    "objectID": "weeks/week15.html#part-ii-content",
    "href": "weeks/week15.html#part-ii-content",
    "title": "DATA 202 - Week 15",
    "section": "Part II: Content",
    "text": "Part II: Content\nLive coding help as needed.",
    "crumbs": [
      "Weekly materials",
      "Week 15"
    ]
  },
  {
    "objectID": "weeks/week15.html#part-iii-code",
    "href": "weeks/week15.html#part-iii-code",
    "title": "DATA 202 - Week 15",
    "section": "Part III: Code",
    "text": "Part III: Code\nData analysis examples:\nThere is a wealth of information on the web to support your future work in R/RStudio. These sources are useful for analyzing data that you find online and want to perform various tests on. Below, I outline some of these sources that can help extend your analysis as you move forward. I will also note that many of these examples are better selections than current AI software and sites (e.g., ChatGPT).\n\nUsing R for Multivariate Analysis\nAn Introduction to Political and Social Data Analysis Using R\nModern Statistics with R\nR for Graduate Students\nR for Social Scientists\nData Analysis Examples",
    "crumbs": [
      "Weekly materials",
      "Week 15"
    ]
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "In this course, students will develop an understanding of statistics as a research tool. Students are expected to have some basic knowledge of statistics from a prior course. Emphasis will be placed on understanding statistical concepts and applying and interpreting tests of statistical inference for real-life applications. The content will include, but not be limited to, visual representations of data, descriptive statistics, correlation and simple regression, sampling distributions, and the assumptions associated with and the application of selected inferential statistical procedures. Throughout the course, there will be a strong emphasis on how statistical modeling can be a driving force for social justice.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-description",
    "href": "weeks/week01.html#course-description",
    "title": "DATA 202 - Week 1",
    "section": "",
    "text": "In this course, students will develop an understanding of statistics as a research tool. Students are expected to have some basic knowledge of statistics from a prior course. Emphasis will be placed on understanding statistical concepts and applying and interpreting tests of statistical inference for real-life applications. The content will include, but not be limited to, visual representations of data, descriptive statistics, correlation and simple regression, sampling distributions, and the assumptions associated with and the application of selected inferential statistical procedures. Throughout the course, there will be a strong emphasis on how statistical modeling can be a driving force for social justice.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-learning-objectives",
    "href": "weeks/week01.html#course-learning-objectives",
    "title": "DATA 202 - Week 1",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\n\nAppreciate and understand the role of statistics in your field\nEvaluate, comprehend, and explain the statistical findings in a data set\nExplore data in R\nApply appropriate application and interpretation of various inferential statistical procedures\nWrite a simple description of methodology and results from analysis\nDevelop an ability to apply appropriate statistical methods to summarize and analyze data\nMake sense of data and be able to report the results in appropriate tables or statistical terms",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-canvas-site",
    "href": "weeks/week01.html#course-canvas-site",
    "title": "DATA 202 - Week 1",
    "section": "Course CANVAS site",
    "text": "Course CANVAS site",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#course-companion-site",
    "href": "weeks/week01.html#course-companion-site",
    "title": "DATA 202 - Week 1",
    "section": "Course companion site",
    "text": "Course companion site\nThe landing page for our companion site can be found here.",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#initial-assignments",
    "href": "weeks/week01.html#initial-assignments",
    "title": "DATA 202 - Week 1",
    "section": "Initial assignments",
    "text": "Initial assignments\n\nPaper #1\nREMINDER: Paper 1 is due Friday September 25\nYou can read more about paper #1 here.\nSee the instructions for paper assignments here.\nAlso, learn more about paper assignments here.\n\n\nCase Study: Tuskegee Experiement of Untreated Syphillis\nTo help you prepare for our forthcoming discussions and readings, you should explore information about our first case study. One place to start is here: “The Tuskegee Experiment: Crash Course Black American History #29” in the video below:",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#references",
    "href": "weeks/week01.html#references",
    "title": "DATA 202 - Week 1",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Weekly materials",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain.\n\n\n\n\n\n\n\n\n\n\nThe Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline.\nLearn more about the Howard University IRB at the Office of Regulatory Research Compliance.\n\n\n\n\nWas the Syphilis Study at Tuskegee an experimental or an observational study? Explain.\n\n\n\nWas data collected for this study using probability or non-probability sampling methods?\n\n\n\nBased on your knowledge of the researchers for the USPHS Syphilis Study at Tuskegee, what was the most likely sampling method used to gather data? Explain.\n\n\n\nBased on your knowledge of the study and the data tables below, list five variables that were collected during the study and explain their variable types in detail.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#case-study",
    "href": "weeks/week03.html#case-study",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "Tuskegee Study of Untreated Syphilis in the Negro Male\nWe begin by exploring a critical historical issue in statistics: understanding the ethics of a medical intervention or study.\n\n\n\nUninformed participants of the Tuskegee Study of Untreated Syphilis in the Negro Male\n\n\n“In 1932, the USPHS, working with the Tuskegee Institute, began a study to record the natural history of syphilis. It was originally called the”Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”). The study initially involved 600 Black men – 399 with syphilis, 201 who did not have the disease. Participants’ informed consent was not collected.” (Office of Science, Centers for Disease Control and Prevention, 2022)\n\nAll images are from Examining Tuskegee by Susan Reverby.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis case study will help frame our understanding between ethics and statistics. The study will allow us to explore longstanding injustices, consider ethical practices in historical contexts, and explore the sociology of statistics. More specifically, we will use this case to understand the history of the Institutional Review Board (IRB) and discuss concepts related to scientific racism. These explorations will help in two key ways: first, we will come to understand what it can mean to be critical in the context of statistics and, second, we will be prepared to respond to problems focused on main concepts in the first few weeks of our course. We will then formalize a few terms.\n\nFraming a relationship between ethics and statistics\nThe “Tuskegee Study of Untreated Syphilis in the Negro Male” (now referred to as the “USPHS Syphilis Study at Tuskegee”) took place in Macon County, Alabama, in an area known as the “Black Belt” because of its rich soil and vast number of Black sharecroppers who were the economic backbone of the region. The research took place at the Tuskegee Institute.\n\nPurpose of the study\nThe intent of the study was to record the natural progression of syphilis in Black men. The study was related to a 1928 retrospective study, the “Oslo Study of Untreated Syphilis,” which reported on the pathological manifestations of untreated syphilis in several hundred white males. However, this original study used secondary data to piece together findings. When the study was initiated in the U.S., there were no proven treatments for the disease. Researchers told the men participating in the study that they were to be treated for “bad blood.” This term had been used by local community members to describe a host of ailments that could be diagnosed including things like anemia, fatigue, and syphilis.\n\nStudy participants\nA total of 600 men were enrolled in the study.\n\nOf this group 399, who had syphilis were a part of the experimental group, and 201 were control subjects.\nMost of the men were poor and illiterate sharecroppers from the county.\nThe participants were offered medical care and insurance.\nThey were enrolled in the study with incentives as well, including medical exams, rides to and from the clinics, meals on examination days, free treatment for minor ailments and guarantees that provisions would be made after their deaths in terms of burial stipends paid to their survivors.\n\n\nEthical issues\n\n\nThere were no proven treatments for syphilis when the study began in 1932.\nWhen penicillin became the standard treatment for the disease in 1947 the medicine was withheld as a part of the treatment for both the experimental group and control group.\nOn July 25, 1972, Jean Heller of the Associated Press broke the story that appeared in both New York and Washington, that there had been a 40-year non-therapeutic experiment called “a study” on the effects of untreated syphilis on Black men in the rural South.\nBetween the start of the study in 1932 and 1947, the date when penicillin was determined as a cure for the disease, dozens of men had died and their wives, children and untold number of others had been infected.\nThere was evidence that scientific research protocol routinely applied to human subjects was either ignored or deeply flawed to ensure the safety and well-being of the men involved. Specifically, the men were never told about or offered the research procedure called informed consent.\n\n\n\nResearchers had not informed the men of the actual name of the study, its purpose, and potential consequences of the treatment or non-treatment that they would receive during the study. The men never knew of the debilitating and life-threatening consequences of the treatments they were to receive, the impact on their partners and children they may have conceived once involved in the research. The panel also concluded that there were no choices given to the participants to quit the study when penicillin became available as a treatment and cure for syphilis. Reviewing the results of the research the panel concluded that the study was “ethically unjustified.” The panel articulated all the above findings in October of 1972 and then one month later the Assistant Secretary for Health and Scientific Affairs officially declared the end of the Tuskegee Study.\n\nClass-Action Suit\nIn the summer of 1973, Attorney Fred Gray filed a class-action suit on behalf of the men in the study, recognized partners, children, and families. It ended a settlement giving more than $9 million to the study participants. Despite these reparative measures, the effects remain.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#institutional-review-board-irb",
    "href": "weeks/week03.html#institutional-review-board-irb",
    "title": "DATA 202 - Week 3",
    "section": "",
    "text": "The Tuskegee Syphilis Study conducted by the U.S Public Health Service was only one of many other past abuses that included unethical experimentation on marginalized groups. As a result of these injustices, a set of mandates were instituted and policies are governed under the IRB, which define the rules and regulations for the approval of research activities. Other countries have equivalent measures focused on ethics.\nAdditional details about can be found online.\nLearn more at https://www.cdc.gov/tuskegee/timeline.\nLearn more about the Howard University IRB at the Office of Regulatory Research Compliance.\n\n\n\n\nWas the Syphilis Study at Tuskegee an experimental or an observational study? Explain.\n\n\n\nWas data collected for this study using probability or non-probability sampling methods?\n\n\n\nBased on your knowledge of the researchers for the USPHS Syphilis Study at Tuskegee, what was the most likely sampling method used to gather data? Explain.\n\n\n\nBased on your knowledge of the study and the data tables below, list five variables that were collected during the study and explain their variable types in detail.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#theory",
    "href": "weeks/week03.html#theory",
    "title": "DATA 202 - Week 3",
    "section": "Theory",
    "text": "Theory\nSome definitions:\n\nPer the Oxford Languages dictionary, a theory is a supposition or a system of ideas intended to explain something, especially one based on general principles independent of the thing to be explained.\nPer Britannica, a theory is an idea or set of ideas that is intended to explain facts or events.\n\nWe will need to think more acutely about theory development as we progress through the term.\n\nFor now, how might you define a theory?",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#theory-construction",
    "href": "weeks/week03.html#theory-construction",
    "title": "DATA 202 - Week 3",
    "section": "Theory construction",
    "text": "Theory construction\nPer Markovsky & Webster (2015), theory construction is the process of formulating components of a theory into a logical whole.\nWe may consider some of the following elements as we prepare theoretical statements.\n\n\nResearch inquiry\nHypotheses\nAnalysis\nEvaluation\nRevision\n\n\n\nFraming data and information\nThere are many different ways to conceptualize data and information.\n\n\nDepending on our specific context, statistical needs, or research purposes, we can frame information as data, or data as information, or even place an equivalence statement between the two terms such that we have: \\[\\text{information} = \\text{data}\\]\nAs we continue to explore what it should mean to be critical in the context of statistics, we will need some common language and base definitions to understand the processes involved in a statistical study.\nWe defined statistics as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information. “Collecting” and “organizing” – the first two steps of this process – requires that we define what we mean by data and information.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#data-and-information",
    "href": "weeks/week03.html#data-and-information",
    "title": "DATA 202 - Week 3",
    "section": "Data and information",
    "text": "Data and information\n\n\n\n\n\n\nDEFINITION: Data\n\n\n\nData are collections of information or observations.\nThe term data is plural, so we should say “data are…” not “data is…”.\nA single data value is referred to as a datum but this term is very rarely used.\n\n\n\n\nThe term “information” is a universal concept that is highly useful but it also lacks precision.\nIn every day terms, information is defined rather loosely.\nIn statistics, however, we might contend that information becomes data when it is collected and organized in some form or fashion.\nThus, it takes some structure to turn information into usable data.\n\n\n\nOne method of organizing collections of data or information is in a Microsoft Excel sheet.\n\n\n\nExcel spreadsheet\n\n\n\nA spreadsheet in a traditional format contains important features that will support data analysis.\n\n\nColumns are vertical arrangements of sets.\n– Columns are represented by capital letters: \\(A\\), \\(B\\), \\(C\\), and so on.\n– We relate columns to sets, which we defined as collections of elements or items.\nRows are horizontal arrangements of elements.\n– Rows are represented by numbers and relate to the index of a set.\n– The subscript \\(i \\in \\mathbb{N}\\) such that \\(i = 1, 2, 3, ...\\), indicates an element’s position.\n\n\n\n\n\n\nExcel spreadsheet with data on US states\n\n\n\n\n\n\n\n\n\nDEFINITION: Filename extension\n\n\n\nA file extension (or file name extension) is a suffix at the end of a digital file name:\n\n\n.doc\n.docx\n.txt\n\nThe extension indicates the file’s format and the organization of information.\n\n\n\n\nThere are many different file extensions.\n\nFor example, the file extension for an MS Excel document can be .xlxs, .csv, or .htm.\nCSV (.csv) files remove all formatting.\n– The removal of formatting helps to reduce errors when transferring data between computers or software programs.\n“CSV” stands for comma-separated values.\n\n\n\n\n\n\nCSV file with data on US states\n\n\n\n\nFormatting data into a data frame\n\nAlthough the CSV format removes formatting, the file is not yet in a structure that we can use to conduct efficient statistical analyses.\nThe CSV format supports a user with collecting and organizing information into a usable data structure.\nHowever, the data needs to be sent to a computer program to undergo the next step in our statistical process: analyzing data. Specifically, we re-format the CSV file to a data frame to conduct analyses.\n\n\n\n\n\n\n\n\nDEFINITION: Data frame\n\n\n\nA data frame (or dataframe) is a two-dimensional table of rows and columns.\n\n\nData frames are a common and popular way to structure, store, and share data.\n\nData frames allow analysts to store sets of observations that vary in size and content.\nEach row describes a single observation.\nEach column stores information for one set, or variable.\n\n\nRecall that a set is a collection of elements.\n\nWe extend this definition to say that a set is a collection of \\(n \\in \\mathbb{N}\\) elements, where \\(n\\) refers to the number of elements in the set.\n\n\n\n\n\n\n\nNote – Sets with \\(n\\) elements\n\n\n\nIf \\(X\\) is a set with \\(n\\) elements, then \\(X\\) can be represented as \\(X = \\{x_1, x_2, x_3, ..., x_n \\}\\).\nIf \\(Y\\) is a set with \\(n\\) elements, then \\(Y\\) can be represented as \\(Y = \\{y_1, y_2, y_3, ..., y_n \\}\\).\n\n\nThe value of \\(n\\) can be used to represent the “size” of a set.\n\nA set with no elements is referred to as the empty set and can be represented as \\(\\{ \\emptyset \\}\\).\n\nWe will need to make sense of the various objects that a set can contain.\n\nFirst, we think more mathematically about sets and data frames.\n\n\n\n\n\n\nDEFINITION: Matrix\n\n\n\nIn mathematics, a matrix is a rectangular table of entries arranged in rows and columns.\n\n\n\nA data frame containing only numbers is an \\(n \\times m\\) matrix:\n\n\\(n\\) refers to the number of rows (or observations)\n\\(m\\) refers to the number of columns (or variables)\n\nBy formatting a series of sets into a data frame, we get\n\n\\(n\\) rows (each row containing data on a single observation)\n\\(m\\) columns (which contain elements over a variable’s values)\n\n\n\nData frame containing \\(n\\) observations for \\(2\\) variables (\\(X\\) and \\(Y\\))\n\n\n\n\n\n\nEXAMPLE – A data frame containing two variables (or sets)\n\n\n\n\n\n\nX\nY\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(x_3\\)\n\\(y_3\\)\n\n\n.\n.\n\n\n.\n.\n\n\n.\n.\n\n\n\\(x_n\\)\n\\(y_n\\)\n\n\n\n\n\n\nTake, for example, two sets with the following ordered values:\n\n\nLet \\(X\\) contain the odd values 1, 3, 5, and 7.\n\nWe have \\(X = \\{1, 3, 5, 7 \\}\\)\n\nLet \\(Y\\) contain the even values 2, 4, 6, and 8.\n\nWe have \\(Y = \\{2, 4, 6, 8 \\}\\)\n\n\n\n\nWe set the following to be true:\n\n\\(x_1 = 1\\), \\(x_2 = 3\\), \\(x_3 = 5\\), \\(x_4 = 7\\)\n\\(y_1 = 2\\), \\(y_2 = 4\\), \\(y_3 = 6\\), \\(y_4 = 8\\)\n\nBy combining the sets \\(X\\) and \\(Y\\), we create the following data frame:\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nX\nY\n\n\n\n\n1\n2\n\n\n3\n4\n\n\n5\n6\n\n\n7\n8\n\n\n\n\n\n\nWe list the index of each element of each set using a new column – ID (for index).\n\n\n\n\n\n\nEXAMPLE – A data frame containing a few odd and even numbers\n\n\n\n\n\n\nID\nX\nY\n\n\n\n\n1\n1\n2\n\n\n2\n3\n4\n\n\n3\n5\n6\n\n\n4\n7\n8\n\n\n\n\n\nThis \\(n \\times m\\) data frame contains\n\n\n\\(n = 4\\) observations\n\\(3\\) variables\n\nID\n\\(X\\)\n\\(Y\\)\n\n\n\n\nWe can consider this structure more generally as noted below.\n\n\n\nStructure of a data set from R for Data Science by Wickham & Grolemund (2022)",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#population-and-sample",
    "href": "weeks/week03.html#population-and-sample",
    "title": "DATA 202 - Week 3",
    "section": "Population and sample",
    "text": "Population and sample\n\n\n\n\n\n\nDEFINITIONS: Population and sample\n\n\n\nA population is representative of every member of a group of interest or collection of objects.\nA sample is a sub-collection of members or objects from a selected population.\n\n\n\n\n\nPopulation and sample. Image from SimplyPsychology.org\n\n\n\n\nThe difference between big \\(N\\) (a population) and little \\(n\\) (a sample)\n\n\n\n\n\n\nNote\n\n\n\n\nWhen referring to the size of a population, use a capital \\(N\\).\nWhen referring to the size of a sample, use a lowercase \\(n\\).\n\n\n\n\n\n\nPopulation parameter vs. Sample statistic\n\n\n\n\n\n\nDEFINITIONS: Parameter versus statistic\n\n\n\nA parameter is a numerical measurement describing some characteristic of a population.\n\nThink of a “population parameter” to remember this relationship.\n\nA statistic is a numerical measure describing some characteristic of a sample.\n\nThink of a “sample statistic” to remember this relationship.\n\n\n\n\nStatistics vs. A statistic\n\n\n\n\n\n\nNote – Statistics (plural) versus a statistic (singular)\n\n\n\nThere is a difference between the term statistics (plural) and a statistic (singular).\n\nWe previously defined statistics (with an ‘s’ at the end) as the science of collecting, organizing, analyzing, interpreting, communicating, and visualizing data and information.\nA statistic (no ‘s’ at the end) refers to a measurement or value from a test on a sample.\n\n\n\n\n\n\nCensus\n\n\n\n\n\n\nDEFINITION: Census\n\n\n\nA census is the collection of data from every member of a population.\n\n\n\n\n\nPopulation census. Image from CasaNC.org\n\n\n\nThere are many sites with free and publicly available data that can help us make better sense of how others have collected data on populations and samples.\n– Sampling of websites with freely accessible data (no downloads required)\n\nColored Conventions Project\nThe DataHub\nData.gov\nKaggle\n\nIn our course, we will focus on integrating how we structure and analyze data by using various mathematical concepts to frame the process of conducting a statistical study.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#what-is-a-variable",
    "href": "weeks/week03.html#what-is-a-variable",
    "title": "DATA 202 - Week 3",
    "section": "What is a variable?",
    "text": "What is a variable?\n\n\n\n\n\n\nDEFINITIONS: Variable\n\n\n\nA variable is any characteristic or quantity that can be measured or counted.\n\n\n\nThere are many types of data and variables used in statistics.\n\nThe type of variable helps determine the appropriate methods for analysis.\n– Categorical or Quantitative\nThe level of measurement helps determine how we measure variables.\n– Nominal, Ordinal, Interval, Ratio\n\n\n\n\nVariable types\nThere are two main variable types: categorical and quantitative variables.\n\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Categorical)\n  A[Variable type] --&gt; C(Quantitative)\n\n\n\n\n\n\n\n\n\n\n\n\nDEFINITIONS: Variable types\n\n\n\nA categorical variable consists of qualitative values such as names or labels.\nA quantitative variable consists of numbers representing counts or measurements.\n\n\n\nData that are categorical in nature are non-numeric.\n– Categorical values are labels used to represent categories or data values.\nData that are quantitative in nature are numeric.\n– Quantitative values make use of the sets of numbers to represent counts or measures.\n\n\nWe can further distinguish between these categories using the following definitions:\n\n\n\n\n\n\nDEFINITIONS: Nominal, Ordinal, Discrete, Continuous\n\n\n\nNominal data are categorical data that cannot be arranged in some order (such as low to high). Some examples are eye color and pet names.\nOrdinal data can be arranged in some order but differences between the values are meaningless. Consider letter grades: A is higher than B but A minus B does not make sense.\nDiscrete data are data where the number of values is finite or “countable.” Some examples include number of students or days spent in the library.\nContinuous data can take on infinitely many values where the collection of values is not countable. Some examples include height or weight.\n\n\n\n\n\n\n\nflowchart LR\n  A[Variable type] --&gt; B(Qualitative)\n  A[Variable type] --&gt; C(Quantitative)\n  C --&gt; D(Discrete)\n  C --&gt; E(Continuous)\n  B --&gt; F(Nominal)\n  B --&gt; G(Ordinal)\n\n\n\n\n\n\n\n\n\nLevels of measurement\nLevels of measurement are used to describe variable types.\n\n\n\n\n\n\n\n\nLevel of measurement\nBrief description\nExamples\n\n\n\n\nNominal\nData cannot be arranged in some order. Only categories are used.\nEye color, city\n\n\nOrdinal\nData can be arranged in some order but differences cannot be found or are meaningless.\nRankings, likert scale\n\n\nInterval\nThere is not a natural zero starting point and rations are meaningless.\nTemperatures, years\n\n\nRatio\nThere is a natural zero starting point and ratios are meaningful.\nHeights, distances\n\n\n\n\n\n\n\nThe four levels of measurement. Image from Scribbr",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-0-understanding-the-rstudio-ide",
    "href": "weeks/week03.html#task-0-understanding-the-rstudio-ide",
    "title": "DATA 202 - Week 3",
    "section": "Task 0: Understanding the RStudio IDE",
    "text": "Task 0: Understanding the RStudio IDE\n\n\n\nRStudio IDE",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-1-create-an-rmarkdown-file-.rmd",
    "href": "weeks/week03.html#task-1-create-an-rmarkdown-file-.rmd",
    "title": "DATA 202 - Week 3",
    "section": "Task 1: Create an RMarkdown file (.Rmd)",
    "text": "Task 1: Create an RMarkdown file (.Rmd)\nWe will conduct most of our work using what is called an RMarkdown.\nThe RMarkdown files allows us to save and annotate our code for future use.\nWhen you run code from an RMarkdownt, it will show up in the Console (bottom left pane in RStudio).\nIn the RStudio IDE, open an RMarkdown by using the following navigation:\n\nFile &gt; New File &gt; R Markdown\n\n\n\nUsing the RMarkdown file\n\n\nPreamble\nThe preamble begins at the top of your RMarkdown file with three dashes.\n\n# ---\n# title: \"Title goes here\"\n# bibliography: references.bib # include references file\n# csl: apa.csl # include cls file for your references\n# always_allow_html: true\n# output:\n#  word_document: \n#    reference_docx: \"word-styles-reference.docx\" # include style file\n#    fig_caption: true\n#  pdf_document:\n#    toc: false\n#    toc_depth: 2\n#    number_section: true \n#  html_document:\n#    toc: true\n#    toc_depth: 2\n#    number_sections: true\n#    theme: flatly\n# geometry: margin=1.0in\n# editor_options:\n#  markdown:\n#    wrap: sentence\n# ---\n\n\n\nSetup\nThe set-up chunk gives the file further instructions.\n\n# ```{r setup, include=FALSE}\n# knitr::opts_chunk$set(\n#  echo = FALSE, # By default, hide code; set to TRUE to see code\n#  fig.pos = 'th', # Places figures at top or here\n#  out.width = '100%', dpi = 300, # Figure resolution and size\n#  fig.env=\"figure\"\n# ) # Latex figure environment\n\n# install.packages(\"praise\")\n# library(praise)\n\n# layout=\"l-body-outset\"\n# library(rmarkdown)\n\n# options(knitr.table.format = \"latex\") # For kable tables to write LaTeX table directly\n# ```\n\n\n\nInserting code into an RMarkdown\n\ny &lt;- 2 + 2\n\n\n# You can use the `#` symbol to leave notes above your code.\ny &lt;- 2 + 2 # You can use the `#` symbol to leave notes in-line with your code.\n\nWe just defined an object y. We can see its value by running this syntax and typing y into the Console.\n\ny\n\n[1] 4\n\n\nThe output on your screen should match the last line above - with the hashtags.\n\n[ 1 ] indicates a single line of results.\nThe output tells us that y has a value of 4, so we say that y is numeric or that it has a numeric value.\n\nYou can run code by clicking ‘Run’ at the top of the Source window, or by typing CMD+Enter",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-2-explore-different-object-types",
    "href": "weeks/week03.html#task-2-explore-different-object-types",
    "title": "DATA 202 - Week 3",
    "section": "Task 2: Explore different object types",
    "text": "Task 2: Explore different object types\nFor this task, we will explore three object types: numeric, character, and logic values.\n\nTask 2-a: Compute a mathematical statement and create a numeric variable\n\n1 + 2\n\n[1] 3\n\n\nWe can assign a variable to this statement by using an assignment operator: &lt;-\n\na &lt;- 1 + 2\n\nWe can also use an equal sign to assign values: \\(=\\)\n\na = 1 + 2\n\nType “a” to show the value of the variable\n\na\n\n[1] 3\n\n\n\nCreate a numeric variable “b” that is the product of “a” and “y”\n\nb = a*y\n\nType “b” in your console to show the product of the two variables\n\nb\n\n[1] 12\n\n\nDivide b by 4\n\nb / 4\n\n[1] 3\n\n\nTake the square root of b\n\nsqrt(b)\n\n[1] 3.464102\n\n\nCompute the natural log of b\n\nlog(b)\n\n[1] 2.484907\n\n\nCompute the common log of b\n\nlog10(b)\n\n[1] 1.079181\n\n\nFind 1 minus the square root of b\n\n1-sqrt(b)\n\n[1] -2.464102\n\n\n\nAttempt to find the square root of “1 minus the square root of b” - which is a negative value\n\nsqrt(1-b)\n\nWarning in sqrt(1 - b): NaNs produced\n\n\n[1] NaN\n\n\nNaN stands for “Not a number”. This occurs because there is currently no defined value to recognize the square root of negative numbers in R. But we can compute the square root on the absolute value of this difference, if needed.\n\nsqrt(abs(1-b))\n\n[1] 3.316625\n\n\n\nWe can insert longer or more complex mathematical statements too. For example, we can find the absolute value of the sum of -1 and the square root of b cubed and then subtract from that the value of 3 times the square root of b.\nNotice the use of parentheses.\n\nabs(-1+sqrt(b^3)) - 3*(sqrt(b))\n\n[1] 30.17691\n\n\nWe can override the original value of y to match the mathematical statement we generated above.\n\ny &lt;- abs(-1+sqrt(b^3)) - 3*(sqrt(b))\ny\n\n[1] 30.17691\n\n\nWe consider all of the previous objects to be numeric.\n\n\n\nTask 2-b: Create a non-numeric value\nWe can also create objects to hold non-numeric values.\nThere are two types of non-numeric values: character values and logic values.\n\nCharacter values\n\ncharacter &lt;- 'some label'\ncharacter\n\n[1] \"some label\"\n\n\nWe can create a character value using the ‘,’ or “,” quotes.\n\ncharacter &lt;- \"some label\"\ncharacter\n\n[1] \"some label\"\n\n\n\n\n\nLogic values\nLogic values can either be TRUE or FALSE\n\nlogic_true &lt;- TRUE\nlogic_false &lt;- FALSE\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE\n\n\nWe can also use T for TRUE and F for FALSE.\n\nlogic_true &lt;- T\nlogic_false &lt;- F\n\n\nlogic_true\n\n[1] TRUE\n\nlogic_false\n\n[1] FALSE",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-3-creating-vectors",
    "href": "weeks/week03.html#task-3-creating-vectors",
    "title": "DATA 202 - Week 3",
    "section": "Task 3: Creating vectors",
    "text": "Task 3: Creating vectors\nWhen we want to list multiple objects or values, we use R’s available data types, such as vectors and factors.\n\nWe concatenate values in these data types using the operator c( ) to place our values in the order we desire.\nConcatenate means to place things together one after the other.\n\n\nVectors\nVectors are a data type we use to order values (i.e., numeric, character, logic) or mix different values.\nFor example, we can store all of the numbers from 1 to 9 in a vector by using a colon.\n\nmy.vector &lt;- c(1:9)\nmy.vector\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\n\nWhen creating vectors, we use the assignment operator and the concatenate option to generate our object.\n\nvec1 &lt;- c(\"WEB Du Bois\", 1868, \"17th\", \"MA\", \"civil rights activist\")\nvec1\n\n[1] \"WEB Du Bois\"           \"1868\"                  \"17th\"                 \n[4] \"MA\"                    \"civil rights activist\"\n\n\nNotice that my numeric and logic values do not use quotations, but a character value uses quotations ““.\n\n\n\nFactors\nFactors are a data type we use to store categorical variables for analyses and data plots. We will explore these later.",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#task-4-creating-data-frames",
    "href": "weeks/week03.html#task-4-creating-data-frames",
    "title": "DATA 202 - Week 3",
    "section": "Task 4: Creating data frames",
    "text": "Task 4: Creating data frames\nFor this final task, we will create a list. Lists can contain anything: functions, vectors, other lists, and data frames.\nTo start, let’s create a series of vectors.\n\nvec1 &lt;- c(\"Ida B. Wells\", \"W.E.B Du Bois\",\"Mary G. Ross\", \"Jaime Escalante\",\"Etta Z. Falconer\", \"Bob Moses\", \"Ruth Gonzales\")\nvec2 &lt;- c(1862, 1868, 1908, 1930, 1933, 1935, 1970)\nvec3 &lt;- c(\"MS\", \"MA\", \"OK\", \"Bolivia\", \"MS\", \"NYC\", \"NJ\")\nvec4 &lt;- c(TRUE, TRUE, TRUE, FALSE, T, T, T)\nvec5 &lt;- c(\"African American\", \"Ghanaian American\", \"Native American\", \"Bolivian American\", \"African American\", \"African American\", \"Mexican American\")\nvec6 &lt;- c(\"Journalist\", \"Sociologist\", \"Engineer\", \"Educator\", \"Mathematician\", \"Educator\", \"Engineer\")\n\n\nOur objects vec1, vec3, and vec5 are made of character values\n\nvec1\n\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\nvec3\n\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\nvec5\n\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\nvec6\n\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nOur object vec2 is made of numeric values\n\nvec2\n\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n\nOur object vec4 is a made of logic values\n\nvec4\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n\n\n\nLists\nLists allow us to do what the name implies - create a list of items.\n\nmy.list &lt;- list(vec1, vec2, vec3, vec4, vec5, vec6)\nmy.list\n\n[[1]]\n[1] \"Ida B. Wells\"     \"W.E.B Du Bois\"    \"Mary G. Ross\"     \"Jaime Escalante\" \n[5] \"Etta Z. Falconer\" \"Bob Moses\"        \"Ruth Gonzales\"   \n\n[[2]]\n[1] 1862 1868 1908 1930 1933 1935 1970\n\n[[3]]\n[1] \"MS\"      \"MA\"      \"OK\"      \"Bolivia\" \"MS\"      \"NYC\"     \"NJ\"     \n\n[[4]]\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE\n\n[[5]]\n[1] \"African American\"  \"Ghanaian American\" \"Native American\"  \n[4] \"Bolivian American\" \"African American\"  \"African American\" \n[7] \"Mexican American\" \n\n[[6]]\n[1] \"Journalist\"    \"Sociologist\"   \"Engineer\"      \"Educator\"     \n[5] \"Mathematician\" \"Educator\"      \"Engineer\"     \n\n\nIn this form, lists can be hard to read.\n\n\n\nData frames\nTo solve the issue with the list, we will use our vectors to generate a specific type of list known as a data.frame.\nData frames are a subtype of lists made of vectors of equal lenght.\n\ndata.frame(vec1, vec2, vec3, vec4, vec5, vec6)\n\n              vec1 vec2    vec3  vec4              vec5          vec6\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice the difference in how the information is arranged when data.frame.\n\nLet’s label this data frame and put labels at the top of the list.\n\ndf &lt;- data.frame(a=vec1, b=vec2, c=vec3, d=vec4, e=vec5, f=vec6)\ndf\n\n                 a    b       c     d                 e             f\n1     Ida B. Wells 1862      MS  TRUE  African American    Journalist\n2    W.E.B Du Bois 1868      MA  TRUE Ghanaian American   Sociologist\n3     Mary G. Ross 1908      OK  TRUE   Native American      Engineer\n4  Jaime Escalante 1930 Bolivia FALSE Bolivian American      Educator\n5 Etta Z. Falconer 1933      MS  TRUE  African American Mathematician\n6        Bob Moses 1935     NYC  TRUE  African American      Educator\n7    Ruth Gonzales 1970      NJ  TRUE  Mexican American      Engineer\n\n\nNotice that when I add labels using the equal sign operator, the vec labels dissapear and are replaced by the categorical labels that we insert. Let’s create more appropriate labels for the data we have generated.\n\ndf &lt;- data.frame(name=vec1, \n                 birthyear=vec2, \n                 birthplace=vec3, \n                 USborn=vec4, \n                 nationality=vec5, \n                 occupation=vec6)\n\n\nFor future use, we can view our data frame by just typing its name into our console or typing View(df).\n\ndf\n\n              name birthyear birthplace USborn       nationality    occupation\n1     Ida B. Wells      1862         MS   TRUE  African American    Journalist\n2    W.E.B Du Bois      1868         MA   TRUE Ghanaian American   Sociologist\n3     Mary G. Ross      1908         OK   TRUE   Native American      Engineer\n4  Jaime Escalante      1930    Bolivia  FALSE Bolivian American      Educator\n5 Etta Z. Falconer      1933         MS   TRUE  African American Mathematician\n6        Bob Moses      1935        NYC   TRUE  African American      Educator\n7    Ruth Gonzales      1970         NJ   TRUE  Mexican American      Engineer",
    "crumbs": [
      "Weekly materials",
      "Week 3"
    ]
  },
  {
    "objectID": "papers/papers-instructions.html",
    "href": "papers/papers-instructions.html",
    "title": "Instructions for each paper",
    "section": "",
    "text": "These are detailed instructions for each paper. See the syllabus for the due dates for each paper.\nSee also another document for course papers: More information for course papers\n\n\nPapers #2 through #4 all require that you do original data analysis. All papers should be written as short research reports (try to make the text clear but sound professional, though the research questions/theories may be simple) and should include the elements described below. Please give each paper a title!\nEach paper should be written as a narrative essay. In writing the essay, you should structure it by using section headings as indicated for each paper. This should not detract from the narrative form (it should read like a paper not like answers to questions in a problem set). Each theory (or graph) should be described fully in about a paragraph, and more when necessary. Describe and justify the statistical relationship involving the theoretical variables that will be examined (including the variable’s unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work).\nInclude a graph of some kind along with your opening at the beginning. Next, describe the data that you will be using to examine the theory. Describe the data source and for any survey data used describe the sampling method. For nationally representative data on high school students, I might say something like “The 2009 HSLS longitudinal sample is a two-stage stratified random sample of the nation’s high school students collected at the school level.” This detailed information can usually be found in the codebooks for the data sets you find. Be mindful, then, that it will require that you review these files in detail as it pertains to your own variables, your own analyses, and its relation to the social issue at hand. Theory is important.\nNext, describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and recorded. Present the frequency distributions (so your coding and treatment of “missing data” categories can be checked); if you want to comment on the frequency distributions, you should do so in no more than 2-3 sentences (we are interested more in what comes after the distribution of the members of your sample or population!). Please include all R commands that you executed for each paper, this will be done in Quarto Markdown in Week 3.\nIn your papers, specify the hypotheses for the expected relationships (correlations/regressions) of the variables and present the operational flow graph. Then report the statistical results, describing the tables, etc. and relevant statistics and any additional calculations. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing. What can you conclude about your original theory based upon your analysis? (*Note: Due to space limitations, when presenting exploratory data analysis and in presenting relationships of variables in research and writing, it is often useful to examine and to report simple tables of means or percentages to summarize bivariate relationships as well, especially when these include nominal or ordinal level (categorical) variables. Keep this in mind in doing research and data analysis beyond this course!)",
    "crumbs": [
      "Appendix",
      "Papers",
      "Instructions for each paper"
    ]
  },
  {
    "objectID": "papers/papers-instructions.html#preparing-papers",
    "href": "papers/papers-instructions.html#preparing-papers",
    "title": "Instructions for each paper",
    "section": "",
    "text": "Papers #2 through #4 all require that you do original data analysis. All papers should be written as short research reports (try to make the text clear but sound professional, though the research questions/theories may be simple) and should include the elements described below. Please give each paper a title!\nEach paper should be written as a narrative essay. In writing the essay, you should structure it by using section headings as indicated for each paper. This should not detract from the narrative form (it should read like a paper not like answers to questions in a problem set). Each theory (or graph) should be described fully in about a paragraph, and more when necessary. Describe and justify the statistical relationship involving the theoretical variables that will be examined (including the variable’s unit of analysis and “what’s the story,” in terms of why we care about the issue(s); this is a great place to insert historical work).\nInclude a graph of some kind along with your opening at the beginning. Next, describe the data that you will be using to examine the theory. Describe the data source and for any survey data used describe the sampling method. For nationally representative data on high school students, I might say something like “The 2009 HSLS longitudinal sample is a two-stage stratified random sample of the nation’s high school students collected at the school level.” This detailed information can usually be found in the codebooks for the data sets you find. Be mindful, then, that it will require that you review these files in detail as it pertains to your own variables, your own analyses, and its relation to the social issue at hand. Theory is important.\nNext, describe the specific measures you are using and how they were obtained. Comment on how well the measures fit your theoretical variables (i.e., the face validity of the measures). In the case of survey data, report the full question wordings (verbatim) and specify the response categories and report how the variables were coded and recorded. Present the frequency distributions (so your coding and treatment of “missing data” categories can be checked); if you want to comment on the frequency distributions, you should do so in no more than 2-3 sentences (we are interested more in what comes after the distribution of the members of your sample or population!). Please include all R commands that you executed for each paper, this will be done in Quarto Markdown in Week 3.\nIn your papers, specify the hypotheses for the expected relationships (correlations/regressions) of the variables and present the operational flow graph. Then report the statistical results, describing the tables, etc. and relevant statistics and any additional calculations. Describe to what extent the evidence is consistent with your hypothesis or hypotheses and lends support to your theorizing. What can you conclude about your original theory based upon your analysis? (*Note: Due to space limitations, when presenting exploratory data analysis and in presenting relationships of variables in research and writing, it is often useful to examine and to report simple tables of means or percentages to summarize bivariate relationships as well, especially when these include nominal or ordinal level (categorical) variables. Keep this in mind in doing research and data analysis beyond this course!)",
    "crumbs": [
      "Appendix",
      "Papers",
      "Instructions for each paper"
    ]
  },
  {
    "objectID": "papers/paper3.html",
    "href": "papers/paper3.html",
    "title": "Paper 3",
    "section": "",
    "text": "For review, you may also refer to more information for course papers in two additional documents:\n\nInstructions for papers\nMore information for course papers\n\n\nStep 1: Theory and logic diagram\nFor paper #3, we are interested in exploring theories pertaining to the relationship between two variables. Specifically, we are interested in understanding the association of characteristics. Your paper 3 assignment should align with your lab 3 submissions, although updates may be made.\nNote: For your paper, please remember to address the theoretical relationship that the literature has identified between the conceptual variables selected. That is, before describing the measures and the survey you will be using, explain your theory and why you think that your independent variable is having an effect on your dependent variable.\n\n\nStep 2: Variables, Measurement, Hypothesis\nAll variables and analyses should be described in detail.\n\n\nStep 3: Statistical Analysis\nPlease be sure to utilize analyses on the interactions between multiple variables.\n\n\nStep 4: Conclusion\nFor this paper, please be sure to focus on a solid conclusion based on your statistical analysis.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Paper 3"
    ]
  },
  {
    "objectID": "papers/annotated.html",
    "href": "papers/annotated.html",
    "title": "Annotated Bibliography",
    "section": "",
    "text": "This page will support you with annotated bibliography assignment. In class, we will review how to develop a single file in Markdown format (.Rmd) to produce your annotated bibliography and course papers.\n\nPurpose\nEvery research paper should include a relevant literature review. Literature reviews can help your readers understand the need for your study, outline a paper’s central thesis, and support readers in making sense of closely related results and findings. As a result, literature reviews can come in many different lengths and formats.\nDoing a literature review is similar to doing any kind of research. A literature review should identify a central question, methodology, and also report findings. One effective way to start a literature review is to create an annotated bibliography.\nPrior to starting an annotated bibliography, it is useful to develop a question that will help you explore and better understand theoretical and methodological connections.\n\n\nFraming a question\nLiterature reviews, like most research projects, can begin from a basic question.\n\nDoes income relate to the availability of resources for [a population]?\nAre years of experience for [a sample] related to their perceptions of power?\n\nOnce you have identified a suitable question, you can use keywords to find sources for an annotated bibliography. The library website can be used to search periodicals or sources like the ERIC system, and you can also utilize popular search engines that house similar works, such as Research Rabbit and Google Scholar.\n\n\nAnnotated bibliographies\nAn annotated bibliography is a list of sources about a specific research question or topic. Each source contains a short statement about the contents of the source (e.g., research paper, online article, or other scholarly and/or published works). Annotated bibliographies are not abstracts, although their length and structure may be similar.\nEach source included in an annotated bibliography should begin with an appropriate citation of the source (in APA, MLA, or Chicago style) and a brief description of the purpose and contents of the source, as well as an evaluation or reflection. The inclusion of an evaluation or reflection when summarizing a published source is one effective way to tell the difference between the abstract and the annotation.\nView samples of annotated bibliographies.",
    "crumbs": [
      "Appendix",
      "Papers",
      "Annotated Bibliography"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Wednesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\n\nTable 1: DATA 202 course schedule (fall 2024)\n\n\n\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nAug 21 - Sep 13\n\n\nPart II: Data and people\n4 weeks\nSep 16 - Oct 11\n\n\nPart III: Data and policy\n4 weeks\nOct 14 - Nov 8\n\n\nPart IV: Data in practice\n3 weeks\nNov 11 - Nov 29\n\n\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nMini-projects: Labs reports. [25 points]\nMini-projects: Papers. Four papers [60 points]\nExam. Midterm examination [15 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW #0 and Lab #0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#a-slightly-more-detailed-overview.",
    "href": "course-overview.html#a-slightly-more-detailed-overview.",
    "title": "Course overview",
    "section": "",
    "text": "On the previous page, we ended with the following three points:\n\nThe syllabus provides a high-level view of the course.\nCanvas is the go-to for submissions and provides readings and assignments.\nThis course site provides technical items and code to help you complete your assignments.\n\nWe’ll focus on the syllabus, Canvas, and this companion site to get a detailed overview of our course.\n\n\nA high-level view of the course.\nPlease note: this section is only a summary of our syllabus. The full syllabus is available on our Canvas site here. I have summarized some important items found on the syllabus below:\nInstructor: Nathan Alexander, PhD\n\nContact information:\nEmail: nathan.alexander@howard.edu\nOffice hours: By appointment at https://nathanalexander.youcanbook.me\nCourse information:\nCourse meeting times: Wednesdays at 5:10pm EST (10:10pm UTC)\nCourse meeting location: Zoom (see Canvas)\n\n\n\n\nTable 1: DATA 202 course schedule (fall 2024)\n\n\n\n\n\nCourse component\nWeeks\nDates1\n\n\n\n\nPart I: Statistics and society\n4 weeks\nAug 21 - Sep 13\n\n\nPart II: Data and people\n4 weeks\nSep 16 - Oct 11\n\n\nPart III: Data and policy\n4 weeks\nOct 14 - Nov 8\n\n\nPart IV: Data in practice\n3 weeks\nNov 11 - Nov 29\n\n\n\n\n\n\n\n\n\nCanvas is the go-to for submissions, and where you can find readings and assignment rubrics. Here is another link to Canvas, I’ll try to drop these as often as possible. Two things about Canvas: it has your assignments and it has your readings.\n\n\n\nThe four categories for assignments in this course are as follows:\n\nMini-projects: Labs reports. [25 points]\nMini-projects: Papers. Four papers [60 points]\nExam. Midterm examination [15 points]\n\n\n\n\n\n\n\n\n\n\n\nThis course should be viewed as an intense but remote introduction to what we might label “critical statistics” or “social justice statistics”. It will be important that we gain both a conceptual and theoretical understanding of the various concepts we will explore, and then a technical understanding of the complex issues that relate to them. We also want to communicate our ideas. These can be difficult tasks.\nBut I encourage you to explore and to be creative, and to take chances. Errors should mostly remind us that there is always the backspace button. Below are some general expectations to keep us all moving forward as a community:\n\n\nCommunicate early and often\nShow up to class on time\nDo not schedule other meetings during class time\nNo late work will be accepted without prior discussion\n\n\n\n\n\n\nYou can return to this page to remind yourself of the four parts of our course, especially when we dive deep into specific projects. This page will remain static (it will not be updated), so use it as an archive that you can return to; start in this page position if you have questions about the schedule, procedures, expectations (above) …then assignments (next).\n\n\n\nIn the next section, we’ll start with a brief overview of assignments and dive into your first two assignments (ungraded). The HW #0 and Lab #0 assignments will serve two purposes: first, they will help introduce you to the tools we’ll use as we begin to review statistical concepts; second, they will set you up for your first graded assignment coming up in two weeks.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#footnotes",
    "href": "course-overview.html#footnotes",
    "title": "Course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlease note that these are rough estimates↩︎",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page will contain resources to support you in our course.",
    "crumbs": [
      "Appendix",
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#the-organic-chemistry-tutors-channel-on-youtube",
    "href": "resources.html#the-organic-chemistry-tutors-channel-on-youtube",
    "title": "Resources",
    "section": "The Organic Chemistry Tutor’s Channel on YouTube",
    "text": "The Organic Chemistry Tutor’s Channel on YouTube\nDespite the profile’s title, this channel offers a wealth of resources spanning mathematics and science. The resources offer detailed examples and explanations that you can follow along with as you watch the video. The below series of video review basic statistics.",
    "crumbs": [
      "Appendix",
      "Resources",
      "Resources"
    ]
  },
  {
    "objectID": "weeks/week06-slides.html#part-i-context",
    "href": "weeks/week06-slides.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "Part I: Context",
    "text": "Part I: Context"
  },
  {
    "objectID": "weeks/week06-slides.html#part-ii-content",
    "href": "weeks/week06-slides.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatter plots\nCorrelation coefficients\nSimple linear regression"
  },
  {
    "objectID": "weeks/week06-slides.html#part-iii-code",
    "href": "weeks/week06-slides.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\nThe code for this week will prepare you to run analyses for paper 2, which is a short exploration of a data set in the forcats package or other data sets that you select. Recall that lab 2 will guide you through your analyses; a sample RScript and workflow is also provided here."
  },
  {
    "objectID": "weeks/week07.html",
    "href": "weeks/week07.html",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-i-context",
    "href": "weeks/week07.html#part-i-context",
    "title": "DATA 202 - Week 6",
    "section": "",
    "text": "Building theories and empirical inquiries around various injustices requires a solid foundation.\n\nOne method is to read recent articles in peer-reviewed journals.\nAnother method is to explore the various ways one may view injustice.\nFrom a logical perspective, you may test the validity of certain claims.\n\nHow should we define social justice?\n\nIdentify two to three definitions of social justice to share.\n\nLocate a few open source articles or periodicals.\n\nWhat are the similarities between the different definitions?\nWhat are the differences between the different definitions?\n\n\n\n\n\nThere are many different frameworks that have been developed to examine social justice.\nNorth (2006), as one example, discuss three spheres of social justice in the article “More Than Words? Delving Into the Substantive Meaning(s) of Social Justice in Education.” The abstract reads as follows:\n\n“At the dawning of the 21st century, the term”social justice” is appearing in numerous public texts and discourses throughout the field of education. However, and as Gewirtz argued in 1998, the conceptual underpinnings of this catchphrase frequently remain tacit or underexplored. This article elaborates Gewirtz’s earlier “mapping” of social justice theories by examining the tensions that emerge when various conceptualizations of social justice collide and, in turn, their implications for the field of education. By presenting a model of the complex, fraught interactions among diverse claims about social justice, the author seeks to promote continued dialogue and reflexivity on the purposes and possibilities of education for social justice.”\n\n\n\n\nThree Spheres of Social Justice by North (2006)\n\n\nIn her analysis, North deals specifically with three conceptions of social justice and their implications for educational research and practice. These frameworks, however, can be extended over to other fields of study. Consider how this framework might apply to your area of study.\n\nThere are different ways to consider these interrelated systems in your theory construction.\n\n\n\nMacro: Large-scale analyses, typically on systems or observations in the aggregate\nMicro: Smaller-scale analyses, typically on individuals or localized contexts\n\n\n\n\n\nSameness: Considering homogeneous structures or characteristics; similarity\nDifference: Considering heterogeneous structures or characteristics; non-uniformity\n\n\n\n\n\nRedistribution: Primary considerations in economic or material conditions\nRecognition: Primary considerations in cultural or social conditions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-ii-content",
    "href": "weeks/week07.html#part-ii-content",
    "title": "DATA 202 - Week 6",
    "section": "Part II: Content",
    "text": "Part II: Content\nThis week’s topics focus on bivariate analysis.\nThe goal of a bivariate analysis is to understand the relationship between two variables.\nThere are a few common ways to perform bivariate analysis:\n\nEstimating differences in proportions\nScatter plots\nCorrelation coefficients\nSimple linear regression\n\n\n\nSimple linear regression\nA simple linear regression (sometimes referenced as a bivariate regression) is a linear equation describing the relationship between an explanatory variable and an outcome variable.\nThere is an assumption that the explanatory variable influences the outcome variable, and not the other way around.\nTake, for example, a variable \\(y_i\\) which denotes the income of some individual in a sample, and we index this data using \\(i\\) where \\(i \\in \\{1, 2, ..., n\\}\\). We can let some other variable in our data \\(x_i\\) represent the years of education for the same individual. A simple linear regression equation of these variables take the following form: \\[y_i = b_0 + b_{1}x_{i} + e_i\\]\nwhere \\(b_1\\) is the sample estimate of the slope of the regression line with respect to the years of education and \\(b_0\\) is the sample estimate for the vertical intercept of the regression line.\n\n\n\nCorrelation coefficients and scatterplots\nAs a reminder, correlation ranges from -1 to 1. It gives us an indication on two things:\n\nThe direction of the relationship between the two variables\nThe strength of the relationship between the two variables\n\nAny outliers can greatly impact the value of a correlation coefficient.\n\nplot(x,y)\n\n\n\n\n\n\n\n\n\n\n\nEstimating differences in proportions\nWhen generating cross tabulations, we can make sense of a few bivariate tests:\n\nDifferences in proportions\nStandard errors of the difference in proportions\nConfidence intervals for the differences\nT-test for differences in proportions",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  },
  {
    "objectID": "weeks/week07.html#part-iii-code",
    "href": "weeks/week07.html#part-iii-code",
    "title": "DATA 202 - Week 6",
    "section": "Part III: Code",
    "text": "Part III: Code\n\n\nWrite preamble\n\n# ---\n# title: Exploring associations between income and political party in the US\n# subtitle: sample paper 2\n# author: Nathan Alexander\n# course: DATA 202 - fall 2023\n# ---\n\n# research inquiry: does income relate to political party support in the US?\n# data: 2020 sample data from the General Social Survey (GSS)\n# note(s): variables should be mutated and recoded into two levels\n\n\n\n\nstep 0: install packages and load libraries\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\nstep 1: view gss_cat data in the package forcats\n\ngetwd() # check working directory\n?gss_cat # view data documentation\ngss_cat # call data frame\nglimpse(gss_cat) # glimpse data\nsummary(gss_cat) # view summary of data\n\n\n\n\nstep 2: clean and manage data\n\ngss_cat_clean &lt;- gss_cat %&gt;% \n  na.omit() %&gt;% \n  select(year, rincome, partyid) %&gt;% \n  rename(income = rincome) %&gt;% \n  rename(party = partyid)\n\ngss_cat_clean # view cleaned data\n\n\n\n\nstep 3: subset data: year == 2000\n\ndf &lt;- gss_cat_clean %&gt;% \n  filter(year==2000)\n\nhead(df) # view top of data\ntail(df) # view bottom of data\nsummary(df) # check data\n\n\n\n\nstep 4: inspect and transform income variable into two levels\n\n## create a frequency table of each level in the income variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Don't know', 'Refused', and 'Not applicable' levels\ndf &lt;- df %&gt;%  \n  filter(income != \"No answer\",\n         income != \"Don't know\",\n         income != \"Refused\",\n         income != \"Not applicable\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## use levels() function to view levels for income variable\nlevels(df$income)\n\n## create two levels: below $20,000 and above $20,000\ndf &lt;- df %&gt;% \n  mutate(income = fct_recode(income, \n                             \"More than 20000\" = \"$25000 or more\",\n                             \"More than 20000\" = \"$20000 - 24999\",\n                             \"Less than 20000\" = \"$15000 - 19999\",\n                             \"Less than 20000\" = \"$10000 - 14999\",\n                             \"Less than 20000\" = \"$8000 to 9999\",\n                             \"Less than 20000\" = \"$7000 to 7999\",\n                             \"Less than 20000\" = \"$6000 to 6999\",\n                             \"Less than 20000\" = \"$5000 to 5999\",\n                             \"Less than 20000\" = \"$4000 to 4999\",\n                             \"Less than 20000\" = \"$3000 to 3999\",\n                             \"Less than 20000\" = \"$1000 to 2999\",\n                             \"Less than 20000\" = \"Lt $1000\"))\n\n## view a summary of your transformed data frame\nsummary(df)\n\n\n\n\nstep 5: inspect and transform party variable into two levels\n\n## create a frequency table of each level in the party variable\ndf %&gt;% count(party)\n\n## drop 'No answer', 'Independent' and 'Other Party' levels\ndf &lt;- df %&gt;%  \n  filter(party != \"No answer\",\n         party != \"Independent\",\n         party != \"Other party\") %&gt;% \n  droplevels() # use droplevels() to remove levels from variable for factors\n\n## create two levels: 'Republican' and 'Democrat'\ndf &lt;- df %&gt;% \n  mutate(party = fct_recode(party,\n                            \"Republican\" = \"Strong republican\",\n                            \"Republican\" = \"Not str republican\",\n                            \"Republican\" = \"Ind,near rep\",\n                            \"Democrat\" = \"Ind,near dem\",\n                            \"Democrat\" = \"Not str democrat\",\n                            \"Democrat\" = \"Strong democrat\"))\n\n## remove year from data frame\ndf &lt;- df %&gt;% \n  select(-year)\n\n## view a summary of the data to check for any errors\nsummary(df)\n\n\n\n\nstep 6: visualize data\n\n## create a frequency table and bar graph of income \ntable.income = table(df$income)\ntable.income\nbarplot(table.income,\n        main = \"Bar graph of Income\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$income),\n        main = \"Bar graph of Income\",\n        col = \"lightgreen\",\n        xlab = \"Respondent Income\",\n        ylab = \"Frequency\",\n        ylim = c(0,650)) # this y-axis range: (0, 650) works best for my plot\n## create a frequency table and bar graph of party \ntable.party = table(df$party)\ntable.party\nbarplot(table.party,\n        main = \"Bar graph of Party\",\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\")\n## the below code produces the same output as above with specifications\nbarplot(table(df$party),\n        main = \"Bar graph of Party\",\n        col = c(\"red\",\"blue\"),\n        xlab = \"Respondent Party\",\n        ylab = \"Frequency\",\n        ylim = c(0,600)) # this y-axis range: (0, 550) works best for my plot\n## create a stacked bar plot of the proportions\n#### question: which of the two plots do you prefer, why?\nplot(df$income, df$party)\nplot(df$party, df$income)\n## the below code produces similar outputs as above with specifications\nplot(df$party, df$income,\n     main = \"Mosaic Plot of Political Party and Income\",\n     col = c(\"lightyellow\",\"lightgreen\"),\n     xlab = \"Political Party\",\n     ylab = \"Income\")\n\n\n\n\nstep 7: create a basic cross tab for manual calculations\n\n## gather sample size\nn = count(df)\nn\n\n## view a basic cross tabulation\ntable(df$income, df$party)\n\n\n\n\nstep 8: bivariate analysis\n\n## load required libraries (and packages, where needed)\ninstall.packages(\"descr\", repos = \"http://cran.us.r-project.org\")\nlibrary(descr)\n\ninstall.packages(\"Hmisc\", repos = \"http://cran.us.r-project.org\")\nlibrary(Hmisc)\n\n## create a cross tab (list dependent variable in your hypothesis first)\ncrosstab(df$party, df$income)\n\n## add column percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.c=T) # add column percentages\n## add row percentages to the cross tab\ncrosstab(df$party, df$income,\n         prop.r=T) # add row percentages\n\n## get expected frequencies and cell chi-square contributions\ncrosstab(df$party, df$income,\n         expected = T, # get expected values\n         prop.chisq=T) # get chi-square contribution\n\n## get critical value of chi-square, p=.05, df=1\n#### recall: df = (r-1)(c-1)\nqchisq(.05, 1, lower.tail=F)\n\n## get chi-square statistic\nchisq.test(df$party, df$income)\n\n\n\n\nstep 9: describe some initial limitations of analysis\n\n### limitation 1: sampling error\n# data come from a sample and there are likely differences in other samples\n\n### limitation 2: category reductions\n# creating two levels for the variables greatly impacted the diversity of responses\n\n### limitation 3: cases dropped\n# sample was further impacted by the number of values dropped in the analysis\n\n### limitation 4: chi-square test\n# the chi-square test does not tell us about the strength or direction of association\n\n\n\n\nNext up: Week 7",
    "crumbs": [
      "Weekly materials",
      "Week 7"
    ]
  }
]